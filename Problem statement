
Problem Statement :
There are N mountains in linear order where you are given an array of height of length L and height[i] denotes the height of the mountain at ith position and you are also given an integer value k i.e the number of mountains to climb .
You can climb mountains in any order. Each mountain you climb has points associated with it which is calculated using below formula:
points = | height[previousMountainIndex] - height[currentMountainIndex] |
Your task is to maximize the minimum points you earned after climbing k mountains.
Input Format :

The first line of input will contain a single integer N, denoting the length of the array
The next line contains N space-separated integers, denoting the elements of an array height.
The next line contains a single integer K.
Output Format :

    For each test case, output on a new line, i.e Answer to the problem (minimum of all the points earned should be maximum).
Constraints:
2 <= k <= height.length <= 100000
1 <= height[i] <= 1000000000

Note:
     you need to write an algorithm than can run in less than n2 complexity
Example test case:-
Example 1:
// n= 6, heights = [13,5,1,8,21,2], k = 3

Input:
6
13 5 1 8 21 2
3

Output:
8

Explanation: Choose the mountains with the heights [5,13,21].
The maximum point earned by climbing the mountains is: min(|5 - 13|, |13 - 21|, |5 - 21|) = min(8, 8, 16) = 8.
So 8 is the maximum Point that can be achieved.

Example 2:
// n= 3 ,heights = [1,3,1], k = 2

Input:
3
1 3 1
2

Output:
2

Explanation: Choose the mountains with the heights [1,3].
The maximum point earned by climbing the mountains is: min(|1 - 3|) = min(2) = 2.
So 2 is the maximum Point that can be achieved.